{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder MNIST TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mnist image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Concatenate x_train and x_test\n",
    "images = np.concatenate((x_train, x_test))\n",
    "\n",
    "# Flatten images\n",
    "images = images.reshape(images.shape[0], 784).astype('float32')\n",
    "\n",
    "# Normalize images\n",
    "images /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for training and network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for training\n",
    "learning_rate = 0.001\n",
    "epochs = 30000\n",
    "batch_size = 32\n",
    "\n",
    "# network parameters\n",
    "image_dimension = 784 #mnist images are 28*28=784\n",
    "layer_size = 512\n",
    "latent_variable_dimension = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier initialization\n",
    "Weight values have to be initialized. This is done randomly.\n",
    "We need to shrink the variance of the weights which will in turn shrink the variance of the weighted sum.\n",
    "We want the weights to have variance 1/n -> multiply each weight with sqrt(1/n) (sqrt(2/n) if using relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(input_shape):\n",
    "    \"\"\"\n",
    "    Initializes weights following xavier initialization\n",
    "    :param input_shape: list, shape of neural network input\n",
    "    :return: val: matrix, initialized weights\n",
    "    \"\"\"\n",
    "    \n",
    "    val = tf.random_normal(shape=input_shape,\n",
    "                           stddev=1./tf.sqrt(input_shape[0]/2.))\n",
    "    \n",
    "    return val\n",
    "\n",
    "#tf.keras.initializers.glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model weights and biases\n",
    "The neural network has five sets of weights and five biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'encoder': tf.Variable(xavier([image_dimension, layer_size])),\n",
    "           'mean': tf.Variable(xavier([layer_size, latent_variable_dimension])),\n",
    "           'std': tf.Variable(xavier([layer_size, latent_variable_dimension])),\n",
    "           'decoder_hidden': tf.Variable(xavier([latent_variable_dimension, layer_size])),\n",
    "           'decoder': tf.Variable(xavier([layer_size, image_dimension]))}\n",
    "\n",
    "biases = {'encoder': tf.Variable(xavier([layer_size])),\n",
    "          'mean': tf.Variable(xavier([latent_variable_dimension])),\n",
    "          'std': tf.Variable(xavier([latent_variable_dimension])),\n",
    "          'decoder_hidden': tf.Variable(xavier([layer_size])),\n",
    "          'decoder': tf.Variable(xavier([image_dimension]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "In the encoding layer, the image is multiplied with the weights, the bias is added, and this is put through a tanh (hyperbolic tangent) activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for image data\n",
    "image_X = tf.placeholder(tf.float32, shape=[None, image_dimension])\n",
    "\n",
    "# Define the computation in the encoder layer\n",
    "Encoder_layer = tf.add(tf.matmul(image_X, weights['encoder']), biases['encoder'])\n",
    "Encoder_layer = tf.nn.tanh(Encoder_layer)\n",
    "\n",
    "# Define the computation in the hidden mean layer\n",
    "Mean_layer = tf.add(tf.matmul(Encoder_layer, weights['mean']), biases['mean'])\n",
    "\n",
    "# Define the computation in the hidden std layer\n",
    "Std_layer = tf.add(tf.matmul(Encoder_layer, weights['std']), biases['std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent vector Z and the reparametrization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate epsilon from a normal distribution with mean=0 and std=1\n",
    "epsilon = tf.random_normal(shape=tf.shape(Std_layer), dtype=tf.float32, mean=0.0, stddev=1.0)\n",
    "\n",
    "latent_vector = Mean_layer + tf.exp(0.5*Std_layer) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder_hidden_layer = tf.add(tf.matmul(latent_vector, weights['decoder_hidden']), biases['decoder_hidden'])\n",
    "Decoder_hidden_layer = tf.nn.tanh(Decoder_hidden_layer)\n",
    "\n",
    "Decoder_output_layer = tf.add(tf.matmul(Decoder_hidden_layer, weights['decoder']), biases['decoder'])\n",
    "Decoder_output_layer = tf.nn.sigmoid(Decoder_output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(original_image, reconstructed_image):\n",
    "    \"\"\"\n",
    "    The loss function for the VAE. Sum of the reconstruction loss\n",
    "    and the KL divergence loss.\n",
    "    :param original_image:\n",
    "    :param reconstructed_image:\n",
    "    :return: loss: float, the combined loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reconstruction loss, cross-entropy loss\n",
    "    rec_loss = original_image * tf.log(1e-10 + reconstructed_image) + (1-original_image) * tf.log(1e-10 + 1-reconstructed_image)\n",
    "    rec_loss = -tf.reduce_sum(rec_loss, 1)\n",
    "    print('reconstruction loss:', rec_loss)\n",
    "    \n",
    "    # KL divergence loss\n",
    "    KL_div_loss = 1 + Std_layer - tf.square(Mean_layer) - tf.exp(Std_layer)\n",
    "    KL_div_loss = -0.5 * tf.reduce_sum(KL_div_loss, 1)\n",
    "    print('KL divergence loss:', KL_div_loss)\n",
    "    \n",
    "    # Sum of the two losses\n",
    "    alpha = 1\n",
    "    beta = 1\n",
    "    network_loss = tf.reduce_mean(alpha * rec_loss + beta * KL_div_loss)\n",
    "    \n",
    "    return network_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_value = loss_function(image_X, Decoder_output_layer)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(epochs):\n",
    "    index = np.random.choice(images.shape[0], 32, replace=True)\n",
    "    x_batch = images[index]\n",
    "    _, loss = sess.run([optimizer, loss_value], feed_dict = {image_X: x_batch})\n",
    "    if i%100 == 0:\n",
    "        print('Loss: {} at iteration: {}'.format(loss, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
